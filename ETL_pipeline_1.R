#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# NCAP Survey ETL Script 
# Author: Nick Dodds 
# Version: 1.3
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# READ ME -----------------------------------------------------------------
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# You must be connected to the internet AND the zscalar VPN for this script to 
# run. If this is your first time running this script, please install the 
# required libraries when prompted. You can also run the lines of code following
# this note as well. If you encounter any errors while running the code please
# check the following common issues prior to reaching out for assistance: 
# 1) Check your inputs in Step 2 and ensure they are spelled and cased correctly
# 2) Confirm that the DTS master file was updated on the shared drive. 
# 3) Confirm that the raw survey data has been saved with the correct naming 
## schema as the others. 
# Run this line of code to install packages: 
# install.packages(c("tidyverse", "readxl", "writexl", "data.table", "lubridate",
  #  "uuid", "janitor", "unheadr", "naniar"))
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# Step 1: Load Libraries --------------------------------------------------
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
library(tidyverse)  # Includes multiple packages for advanced data manipulation.
library(readxl)     # Allows reading of Excel files (specifically .xlsx).
library(writexl)    # Allows the writing of Excel files.
library(data.table) # Allows more advanced data frame functionality.
library(lubridate)  # Allows easier date field manipulation.
library(uuid)       # Allows assignment of unique IDs.
library(janitor)    # Tidy Variable Names 
library(unheadr)    # Cleaning Nested Variables Names in Columns
library(naniar)     # Dealing with NA values

#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# Step 2: Input the Required Values for Dynamic Fields --------------------
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

# (1) NCAP_SURVEY_TYPE #
# Input format: Either "Recovery" or "Response"  
# NOTE: This is case sensitive.
ncap_survey_type <- "Recovery"

# (2) DISASTER_NUMBER #
# 4-digit DR number (e.g., "4738")
disaster_number <- "4753"

# (3) STATE_CODE #
# Input format: 2-letter state abbreviation, e.g., California = "CA"
state_code <- "RI"

# (4) QUESTION_COLUMN_NAME_TAB #
# Input format: The name of the corresponding column match tab, e.g. 
## YYYY_[Region if applicable, DR if unique]
# If there were Regional questions added to the NCAP Survey, this will need to
## be changed!
# Otherwise, this should default to "2024_Base"
question_column_name_tab <- "2024_Base"

# UUID CRYPTOGRAPHIC KEY #
# THIS IS A STATIC FIELD AND SHOULD NOT CHANGE
## Generated by time on 2024-02-28
DNS.namespace <- "1ea18abc-d669-11ee-8000-9d57bf0070f9" 
# UUIDgenerate(use.time = TRUE)

# DISASTER_ID # 
disaster_id <- paste("DR", disaster_number, sep = "-")

# Generate the Disaster_ID_Long
disaster_id_long <- paste(disaster_id, state_code, sep = "-")

ncap_survey_type_long <- if(ncap_survey_type == "Recovery") {
  "02-RECOVERY"
} else {
  "01-RESPONSE"
}

ncap_survey_type_code <- if(ncap_survey_type == "Recovery") {
  "02"
} else {
  "01"
}

survey_read_file_name <- paste(disaster_id_long,
                               NCAP_SURVEY_TYPE_LONG,
                               sep = "_") %>%
  paste("xlsx", sep = ".")


# SURVEY_WRITE_FILE_NAME # The survey write file name is already dynamic

CORRESPONDING_DTS_MASTER_FILE_NAME <- paste("DTSMaster_PII_",
                                            toupper(ncap_survey_type_long),
                                            ".xlsx", sep = "")
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# Step 3: Load Unprocessed Data -------------------------------------------
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

# Update working directory
setwd("") 

# Load in raw survey data 
## NOTE: Must be .xlsx Excel file for a specific disaster and only one survey's 
## worth of data.
ncap_survey <- read_xlsx(
  path = survey_read_file_name,
  col_names = T
) %>% 
  # Combine nested headers into into a single column
  mash_colnames(n_name_rows = 1) %>% 
  # Tidy Names 
  clean_names() 

# Create error index to be used later 
respondent_count_intitial <- nrow(ncap_survey)

#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# Step 4: Rename Variables ------------------------------------------------
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

# Create an index vector of prefixes that we will add to our column names. 
## These prefixes were created using the existing naming structure established 
## by the original script 
prefixes <- c("WR1", "WR1a", "WR1b", "WR2", "WR3", "WR2a", "WR3a", "WR4a", 
              "WR4b", "WR4c", "WR4d", "WR4e", "WR5", "WR6", "WR7", "WR8",
              "WR8a", "WR9", "WR10", "WR10a", "WR11", "WR12a", "WR12b", "WR13",
              "WR12c", "WR14", "WR15a", "WR15b", "WR15c","WR16a", "WR16b",
              "WR16c", "WR16d", "WR16e", "WR16f", "WR16g", "WR16h", "WR16i",
              "WR17", "WR18", "WR18a", "WR18b", "WR18c", "WR18d", "WR19",
              "WR19a", "WR20", "WR21", "WR22", "WR22a", "WR23a", "WR23b", 
              "WR23c", "WR23d", "WR23e", "WR23f", "WR24", "WR24a", "WR25",
              "WR25a", "WR26a", "WR26b", "WR26c", "WR27", "WR27a", "WR28",
              "WR29", "WR29a", "WR28a", "WR28b", "WR30", "WR30a", "WR30b",
              "WR30c", "WR30d", "WR31", "WR31a", "WR32", "WR33", "WR32a", 
              "WR33a", "WR34", "E1", "E2", "E2a", "E2b", "E2c", "E2d", "E2e",
              "E2f", "E2g", "E2h", "E2i", "E3a", "E3b", "E3c", "E3d", "E4a",
              "E4b", "E4c", "E4d", "E4e", "E4f", "E4g", "E5a", "E5b", "E5c", 
              "E6", "E7", "E7a", "E7b", "E7c", "E7d", "E8", "E9a", "E9b", 
              "RM0a", "RM0b", "RM0c", "RM1a", "RM1b", "RM1c", "RM1d", "RM1e",
              "RM2", "RM3a", "RM3b", "RM3c", "RM3d", "RM3e", "RM3f", "RM3g", 
              "RM3h", "RM3i", "RM4", "RM5a", "RM5b", "RM5c", "RM5d", "RM5e",
              "RM5f", "RM5g", "RM5h", "RM5i", "RM6a", "RM6b", "RM6c", "RM6d",
              "RM6e", "RM6f", "RM6g", "RM6h", "RM7a", "RM7b", "RM7c", "RM7d",
              "RM7e", "RM7f", "RM7g", "RM7h", "RM7i", "RM8", "CQ1", "CQ2a",
              "CQ2b", "CQ2c", "P0", "P-EQ1", "P-EQ2a", "P-EQ2b", "P-EQ2c",
              "P-EQ3a", "P-EQ3b", "P-EQ3c", "P-EQ4a", "P-EQ4b", "P-EQ4c", 
              "P-EQ4d", "P-EQ5a", "P-EQ5b", "P-EQ5c", "P-EQ5d", "P-EQ5e",
              "P-EQ5f", "P-EQ5g", "P-EQ5h", "P-EQ5i", "P-EQ5j", "P-EQ5k", 
              "P-EQ5l", "P-EQ6")


# Rename Columns
ncap_survey <- ncap_survey %>% 
  # Add prefixes to column names
  rename_with(
    ~ paste(prefixes, .x, sep = "_"),
    # Apply to the survey question columns only 
    .cols = 13:ncol(ncap_survey)
  ) %>% 
  # Remove extra prefixes from the OG survey data 
  rename_with(
    ~ str_remove_all(.x, "x[0-9]+(?=_)"), 
    # Apply to the survey question columns only 
    .cols = 13:ncol(ncap_survey)
  ) %>% 
  # Rename the unnamed questions imported from the survey
  rename(
    "disaster_id" = custom_data_1, 
    "ncap_survey_type"= custom_data_2, 
    "survey_close_date" = custom_data_3, 
    "is_reservist" = custom_data_4
  ) %>% 
  # Standardize Disaster ID for join
  mutate(disaster_id = str_remove_all(disaster_id, 
                                      paste("-", state_code, sep = "")))

# Calculates the number of regional questions based on difference from base value
## There should be 203 in the base output from SurveyMonkey
n_regional_questions <- (ncol(ncap_survey) - 203) 

#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# Step 5: Generate UUID's with Emails -----------------------------------------
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

ncap_survey <- ncap_survey %>% 
  mutate(
    # Transform email addresses to lowercase
    email_address = tolower(email_address),
    # Create UUID's 
    uuid = UUIDfromName(DNS.namespace, email_address)
  )
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# Step 6: Import and Prepare DTS Data for Join --------------------------------
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

# Set new working directory
setwd("###")

# Create index vector of column types for DF Import. 
column_types <- c("text", "text", "text", "date", "text", "text", "text",
                  "text", "text", "text", "text", "text", "text", "text",
                  "text", "text", "text", "text", "text", "text", "text", 
                  "text", "text", "text", "text", "text", "text", "text", 
                  "text", "text", "numeric", "date", "date", "text", "text",
                  "text", "text", "text", "numeric", "numeric", "numeric",
                  "date", "date", "date", "date", "date", "date", "text", 
                  "text", "text", "text", "date", "text", "text", "text",
                  "text", "text", "text", "text", "text", "text", "text", 
                  "text", "text", "text", "text", "text", "text", "text")

# Load in DTS Data
dts_subset <- read_xlsx(
  path = CORRESPONDING_DTS_MASTER_FILE_NAME, 
  col_names = T, 
  col_types = column_types, 
  na = c("", NA, "N/A", "#N/A", "na", "Na"), 
  progress = T
) %>% 
  # Tidy names
  clean_names()

# Rename Columns 
dts_subset <- dts_subset %>%
  # Declare the unique transformations
  rename(
    home_office = employee_region, 
    deployed_program_area = deployed_prog_area, 
    on_site_date = on_site, 
    release_date = release, 
    assigned_coach_evaluator = assigned_c_es
  ) %>% 
  # Apply renaming function 
  rename_with(
    ~ paste("dts", .x, sep = "_"),
    # Apply to columns of interest only
    .cols = 6:ncol(dts_subset)
  )

# Data Wrangling
dts_subset <- dts_subset %>% 
  mutate(
    # Calculate the Time deployed
    dts_days_deployed = as.numeric(
      # Convert Seconds to Days 
      dts_release_date - dts_on_site_date, unit = "days"
      # Add one more day (why?)
    ) + 1,
    # Standardize 'Home Office' values
    dts_home_office = case_when(
      dts_home_office == "Region 1" | dts_home_office == "Region I" ~ "R1",
      dts_home_office == "Region 2" | dts_home_office == "Region II" ~ "R2",
      dts_home_office == "Region 3" | dts_home_office == "Region III" ~ "R3",
      dts_home_office == "Region 4" | dts_home_office == "Region IV" ~ "R4",
      dts_home_office == "Region 5" | dts_home_office == "Region V" ~ "R5",
      dts_home_office == "Region 6" | dts_home_office == "Region VI" ~ "R6",
      dts_home_office == "Region 7" | dts_home_office == "Region VII" ~ "R7",
      dts_home_office == "Region 8" | dts_home_office == "Region VIII" ~ "R8",
      dts_home_office == "Region 9" | dts_home_office == "Region IX" ~ "R9",
      dts_home_office == "Region 10" | dts_home_office == "Region X" ~ "R10",
      dts_home_office == "Non FEMA" ~ "NF", 
      T ~ dts_home_office
    ), 
    # Standardize 'Event Region' variable
    dts_event_region = case_when(
      dts_event_region == "Region 1" ~ "R1",
      dts_event_region == "Region 2" ~ "R2",
      dts_event_region == "Region 3" ~ "R3",
      dts_event_region == "Region 4" ~ "R4",
      dts_event_region == "Region 5" ~ "R5",
      dts_event_region == "Region 6" ~ "R6",
      dts_event_region == "Region 7" ~ "R7",
      dts_event_region == "Region 8" ~ "R8",
      dts_event_region == "Region 9" ~ "R9",
      dts_event_region == "Region 10" ~ "R10",
      T ~ dts_event_region
    ), 
    # Dummy Code the 'Assigned Trainee' variable 
    dts_assigned_trainees = case_when(
      !is.na(dts_assigned_trainees) ~ "Yes", 
      T ~ "No"
    ), 
    # Dummy Code the 'Assigned C&E' variable 
    dts_assigned_coach_evaluator = case_when(
      !is.na(dts_assigned_coach_evaluator) ~ "Yes", 
      T ~ "No"
    )
  )

# Subset the columns of interest in desired order 
dts_subset <- dts_subset %>% 
  select(disaster_id, ncap_survey_type, uuid, unique_id, 
         dts_home_office, dts_event_region, dts_employee_type,
         dts_fqs_program_area, dts_fqs_position, dts_fqs_proficiency, 
         dts_deployed_program_area, dts_deployed_position, 
         dts_deployed_proficiency, dts_on_site_date, dts_release_date,
         dts_days_deployed, dts_request_type, dts_is_coach_and_evaluator,
         dts_assigned_trainees, dts_assigned_coach_evaluator)

#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# Step 7: Join and Organize Responder Data ------------------------------------
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

# Join the NCAP Survey and DTS Subset dataframes
ncap_survey <- left_join(
  ncap_survey, dts_subset, 
  by = c("disaster_id", "ncap_survey_type", "uuid") 
)
# Saves a count of the post-join rows for error handling purposes later
respondent_count_final <- nrow(ncap_survey)

# Data Validation
if(respondent_count_intitial == respondent_count_final){
  print(
    "Success! All unprocessed NCAP Survey responses have been matched to their DTS records."
  )
}else{
  stop(
    "Failure! There was a problem in the join. Not all NCAP Survey responses
    match their DTS records and responses have been dropped. Check each
    respondentsUUID, Disaster_ID, and NCAP_Survey_Type fields for potential
    mistmatches."
  )
}

# Data Wrangling
ncap_survey <- ncap_survey %>% 
  mutate(
    # Transform 'Survey Close Date'
    survey_close_date = as_date(survey_close_date, format = "%m/%d/%Y"), 
    # Create the 'NCAP Briefing Quarter' variable
    ncap_briefing_quarter = paste(
      year(survey_close_date), 
      quarters(survey_close_date), sep = "-"
    ), 
    # Add Demographic Variables 
    d1_supervisor_status = 
      WR13_were_you_a_supervisor_on_this_deployment_response, 
    d2_virtual_deployment =
      WR18_did_you_deploy_in_person_or_virtually_to_support_this_incident_response, 
    d3_concurrent_deploments = case_when(
      is.na(CQ1_if_you_supported_other_disaster_s_at_the_same_time_as_this_one_please_list_them_in_the_box_below_otherwise_leave_it_blank_please_list_each_4_digit_dr_number_separated_by_a_space_for_example_1234_5678_9012_open_ended_response) ~ "Yes", 
      T ~ "No"
    )
  ) %>% 
  # Remove Personal Identifying and redundant fields
  select(-c(respondent_id, collector_id, ip_address, email_address,
            first_name, last_name, is_reservist))

# Join the NCAP Survey and DTS Subset dataframes
ncap_survey <- left_join(
  ncap_survey, dts_subset, 
  by = c("disaster_id", "ncap_survey_type", "uuid") 
)
# Saves a count of the post-join rows for error handling purposes later
respondent_count_final <- nrow(ncap_survey)

# Data Validation
if(respondent_count_intitial == respondent_count_final){
  print(
  "Success! All unprocessed NCAP Survey responses have been matched to their DTS records."
  )
}else{
  stop(
    "Failure! There was a problem in the join. Not all NCAP Survey responses
    match their DTS records and responses have been dropped. Check each
    respondentsUUID, Disaster_ID, and NCAP_Survey_Type fields for potential
    mistmatches."
  )
}

# Data Wrangling
ncap_survey <- ncap_survey %>% 
  mutate(
    # Transform 'Survey Close Date'
    survey_close_date = as_date(survey_close_date, format = "%m/%d/%Y"), 
    # Create the 'NCAP Briefing Quarter' variable
    ncap_briefing_quarter = paste(
      year(survey_close_date), 
      quarters(survey_close_date), sep = "-"
    ), 
    # Add Demographic Variables 
    d1_supervisor_status = 
      WR13_were_you_a_supervisor_on_this_deployment_response, 
    d2_virtual_deployment =
      WR18_did_you_deploy_in_person_or_virtually_to_support_this_incident_response, 
    d3_concurrent_deploments = case_when(
      is.na(CQ1_if_you_supported_other_disaster_s_at_the_same_time_as_this_one_please_list_them_in_the_box_below_otherwise_leave_it_blank_please_list_each_4_digit_dr_number_separated_by_a_space_for_example_1234_5678_9012_open_ended_response) ~ "Yes", 
      T ~ "No"
    )
  ) %>% 
  # Remove Personal Identifying Fields and redundant fields
  select(-c(respondent_id, collector_id, ip_address, email_address,
            first_name, last_name, is_reservist))

# Reorder Variables
ncap_survey <- ncap_survey %>% 
  select(
    disaster_id, ncap_survey_type, ncap_briefing_quarter, uuid,
    start_date, end_date, survey_close_date,
    matches("^d[1-9]"), 
    unique_id, 
    starts_with("dts_"), 
    everything()
  )


#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# Step 8: Export Excel Files to Drive -----------------------------------------
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# Set working directory
setwd("") 

# Create index vector for free response questions
free_response_questions <- c("WR5", "WR8a", "WR9", "WR11", "WR14", "WR17",
                             "WR18c", "WR22a", "WR23f", "WR30c", "WR30d",
                             "WR31a", "WR32a", "WR33a", "WR34", "E6", "E8", 
                             "E9a", "E9b", "RM2", "RM4", "RM8", "CQ2a", "CQ2b",
                             "CQ2c", "P-EQ1", "P-EQ6")

# Remove free response questions & create NOPII DF 
ncap_survey_nopii <- ncap_survey %>% 
  select(-starts_with(free_response_questions))

# Remove regional questions and create Core Survey DF 
core_ncap_survey <- if(COUNT_REGIONAL_QUESTIONS > 0){
  ncap_survey %>% 
    select(-starts_with("RQ"))
}else{
  ncap_survey
}

# Index the number of rows in the core survey for data validation later
nrow_incremental <- nrow(core_ncap_survey)

# Remove regional questions and create NOPII core survey
core_ncap_survey_nopii <- if(COUNT_REGIONAL_QUESTIONS > 0){
  ncap_survey_nopii %>% 
    select(-starts_with("RQ"))
}else{
  ncap_survey_nopii
}

# Write incremental survey data files
## With identifying data 
write_xlsx(
  list("Response_PII" = ncap_survey), 
  path = paste0(
    disaster_id, sep = "-", STATE_CODE, sep = "_", 
    ncap_survey_type_long, sep = "_", "PII", ".xlsx"
  ), 
  col_names = T, 
  format_headers = T
)
## Without identifying data 
write_xlsx(
  list("Response_NoPII" = ncap_survey), 
  path = paste0(
    disaster_id, sep = "-", STATE_CODE, sep = "_", 
    ncap_survey_type_long, sep = "_", "NoPII", ".xlsx"
  ), 
  col_names = T, 
  format_headers = T
)

# Read in Master Data Frames
# Set working directory 
setwd("") 
## Full Survey Master DF
full_ncap_master <- read_xlsx(
  "FULL_NCAPSurvey_MasterDataset.xlsx", 
  col_names = T, 
  na = ""
)
## Full Survey Master DF
current_ncap_master <- read_xlsx(
  "CURRENT_NCAPSurvey_MasterDataset.xlsx", 
  col_names = T, 
  na = ""
)
# Index number of rows for data validation 
nrow_before <- nrow(current_ncap_master)

# Write backups 
## Full Master Survey 
full_ncap_master %>% 
  write_xlsx(
    "FULL_NCAPSurvey_MasterDataset_BACKUP.xlsx", 
    col_names = T, 
    format_headers = T
  )
## Current Master Survey 
current_ncap_master %>% 
  write_xlsx(
    "CURRENT_NCAPSurvey_MasterDataset_BACKUP.xlsx", 
    col_names = T, 
    format_headers = T
  )

# Standardize data types across DF's 
## Create index vectors declaring data types for columns 
### Numeric Columns 
numeric_cols <- c("DTS.Days_Deployed")
### Date Columns
date_cols <- c("Survey_Close_Date","DTS.On_Site_Date","DTS.Release_Date")
### Date and Time Columns
datetime_cols <- c("Survey_Start_Time","Survey_End_Time")

## Transform Columns 
### Full NCAP master Survey
full_ncap_master <- full_ncap_master %>% 
  mutate(
    # Numeric Columns
    across(numeric_cols, as.numeric), 
    # Date Columns
    across(date_cols, as.Date), 
    # Date and Time Columns
    across(datetime_cols, as.POSIXct),
    # Character Columns
    across(-c(numeric_cols, date_cols, datetime_cols))
  )
### Current NCAP master Survey
current_ncap_master <- current_ncap_master %>% 
  mutate(
    # Numeric Columns
    across(numeric_cols, as.numeric), 
    # Date Columns
    across(date_cols, as.Date), 
    # Date and Time Columns
    across(datetime_cols, as.POSIXct),
    # Character Columns
    across(-c(numeric_cols, date_cols, datetime_cols))
  )

# Add incremental survey dat to NCAP survey master DF's
## Full Master DF
full_ncap_master <- bind_rows(full_ncap_master, core_ncap_survey)
## Current Master DF
current_ncap_master <- bind_rows(current_ncap_master, core_ncap_survey)

# Index number of rows after adding incremental rows to master DF
nrow_after <- nrow(current_ncap_master)

# Index number of variables in master DF
ncols_master <- ncol(current_ncap_master)

# Data Integrity Checks
## Ensure new rows were added correctly
if(nrow_after == nrow_before + nrow_incremental){
  print(
    "Success! The incremental load to the NCAP Survey master dataset was successful."
  )
}else{
  stop(
    "Failure! There was an issue with adding the incremental dataset to the 
    NCAP Survey Master dataset. Check for a breakdown in the code before and 
    after binding the 'current_ncap_master' to the 'core_ncap_survey'."
  )
}
## Ensure all variables are captured 
if(ncols_master == 218 + n_regional_questions){
  print(
    "Success! The NCAP Survey Master dataset has the correct number of variables."
  )
}else{
  print(
    "Failure! The NCAP Survey Master dataset has the wrong number of variables. Check that all regional column questions were accounted for."
  )
}

## Check for duplicated rows
### Create index vector of responder records
responder_records <- paste0(
  current_ncap_master$Disaster_ID, sep = "-", 
  current_ncap_master$NCAP_Survey_Type, sep = "-", 
  current_ncap_master$Unique_ID
)
### Write a function to ensure all records are unique
is_unique <- function(responder_records){
  return(!any(duplicated(responder_records)))
}

if(is_unique(responder_records) == T){
  print(
    "Success! All records are unique"
  )
}else{
  stop(
    "Failure! There are duplicate records in the master NCAP dataset. Remove them before proceeding."
  )
}


# Export Full and Current Master DF's 
## Full 
full_ncap_master %>% 
  write_xlsx(
    path = "FULL_NCAPSurvey_MasterDataset.xlsx", 
    col_names = T, 
    format_headers = T
  )
## Current
current_ncap_master %>% 
  write_xlsx(
    path = "CURRENT_NCAPSurvey_MasterDataset.xlsx", 
    col_names = T, 
    format_headers = T
  )
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# Step 9: Create Summary Files  -----------------------------------------------
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# Pivot the data longer for the BI Dashboard 
ncap_survey_summary_data <- core_ncap_survey_nopii %>% 
  pivot_longer(
    cols = -c(1:27), 
    names_to = "Question", 
    values_to = "Response"
  )

# Adjust likert scale values 
ncap_survey_summary_data <- ncap_survey_summary_data %>% 
  mutate(
    Response = case_when(
      # Agreement Scale
      Response == 'Strongly Disagree' ~ '[1] Strongly Disagree', 
      Response == 'Disagree' ~ '[2] Disagree',
      Response == 'Neutral' ~ '[3] Neutral',
      Response == 'Agree' ~ '[4] Agree', 
      Response == 'Strongly Agree' ~ '[5] Strongly Agree',
      # Helpfulness Scale
      Response == 'Not at all helpful' ~ '[1] Not at all helpful',
      Response == 'Slightly helpful' | Response == 'Somewhat helpful' ~ 
        '[2] Slightly helpful' ,
      Response == 'Moderately helpful' | Response == 'Helpful' ~ 
        '[3] Moderately helpful', 
      Response == 'Very helpful' ~ '[4] Very helpful',
      Response == 'Strongly Agree' ~ '[5] Strongly Agree',
      Response == 'Extremely helpful' ~ '[5] Extremely helpful',
      # Satisfaction Scale
      Response == 'Completely Dissatisfied' ~ '[1] Completely Dissatisfied',
      Response == 'Dissatisfied' ~ '[2] Dissatisfied',
      Response == 'Neutral' ~ '[3] Neutral',
      Response == 'Satisfied' ~ '[4] Satisfied', 
      Response == 'Completely Satisfied' ~ '[5] Completely Satisfied', 
      # Complete Agreement Scale
      Response == 'Completely Disagree' ~ '[1] Completely Disagree',
      Response == 'Disagree' ~ '[2] Disagree', 
      Response == 'Neutral' ~ '[3] Neutral', 
      Response == 'Agree' ~ '[4] Agree', 
      Response == 'Completely Agree' ~ '[5] Completely Agree', 
      # Staffing Levels 
      Response == 'Heavily Understaffed' ~ '[1] Heavily Understaffed',
      Response == 'Moderately Understaffed' ~ '[2] Moderately Understaffed', 
      Response == 'Slightly Understaffed' ~ '[3] Slightly Understaffed', 
      Response == 'Just Right' | Response == 'Just right' ~ '[4] Just Right', 
      Response == 'Slightly Overstaffed' ~ '[5] Slightly Overstaffed', 
      Response == 'Moderately Overstaffed' ~ '[6] Moderately Overstaffed', 
      Response == 'Heavily Overstaffed' ~ '[7] Heavily Overstaffed', 
      # Months Ago 
      Response == 'Less than 1 month ago' ~ '[1] Less than 1 month ago', 
      Response == '1-3 months ago' ~ '[2] 1-3 months ago', 
      Response == '4-6 months ago' ~ '[3] 4-6 months ago', 
      Response == '7-12 months ago' ~ '[4] 7-12 months ago', 
      Response == '13-24 months ago' ~ '[5] 13-24 months ago', 
      Response == 'More than 24 months ago' ~ '[6] More than 24 months ago', 
      T ~ Response
      
      
    )
  )

# Create Frequency Table of Respondents by Disaster ID and Question
aggregated_data <- ncap_survey_summary_data %>% 
  # Count only those questions which were answered
  filter(!is.na(Response)) %>% 
  ## Tally the number of responses by question and Disaster ID
  group_by(Disaster_ID, Question) %>% 
  summarise(Respondent_Count = n())


# Join frequency table to the Survey Summary DF
ncap_survey_summary_data <- left_join(
  ncap_survey_summary_data,
  aggregated_data, 
  by = c("Disaster_ID", "Question")
) %>% 
  # Remove unnecessary filter fields 
  select(-c(NCAP_Briefing_Quarter, Survey_Start_Time, Survey_End_Time))

# Update Working Directory
setwd("")  

# Load Framework File
ncap_survey_summary_master_old <- read_csv(
  file = "NCAPSurveySummary_SurveyResponsesDataset.csv", 
  col_names = T, 
  na = ""
)

# Create indices for data validation 
nrow_before_summary <- nrow(ncap_survey_summary_master_old)
nrow_incremental_summary <- nrow(ncap_survey_summary_data)


# Write backup "just in case"
write_csv(
  NCAP_SURVEYSUMMARY_MASTER_OLD, 
  file = "NCAPSurveySummary_SurveyResponsesDataset_BACKUP.csv", 
  col_names = T, 
  na = ""
)

# Standardize data types across DF's 
## Create index vectors declaring data types for columns 
### Numeric Columns 
numeric_cols <- c("DTS.Days_Deployed","Respondent_Count")
### Date Columns
date_cols <- c("Survey_Close_Date","DTS.On_Site_Date","DTS.Release_Date") 


## Transform Columns 
### Previous NCAP Summary Master File
ncap_survey_summary_master_old <- ncap_survey_summary_master_old %>% 
  mutate(
    # Numeric Columns
    across(numeric_cols, as.numeric), 
    # Date Columns
    across(date_cols, as.Date), 
    # Character Columns
    across(-c(numeric_cols, date_cols))
  )

# Add new summary survey data to the Master File
ncap_survey_summary_master_new <- bind_rows(
  ncap_survey_summary_master_old, ncap_survey_summary_data
)

# Create index for data validation 
nrow_after_summary <- nrow(ncap_survey_summary_master_new)

# Data Integrity Check
if(nrow_after_summary == nrow_before_summary + nrow_incremental_summary){
  print(
    "Success! New rows were successfully added to the summary mastery file"
  )
}else{
  stop(
    "Failure! There was an issue with adding the new summary data rows to the
    NCAP Survey Summary master file. Check for a breakdown in the code that 
    pivots the data or harmonizes the variables between the datasets."
  )
}

### Write to CSV 
write_csv(
  NCAP_SURVEYSUMMARY_MASTER_NEW,
  file = "NCAPSurveySummary_SurveyResponsesDataset.csv", 
  col_names = T, 
  na = ""
)
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# Step 10: Clear Workspace ----------------------------------------------------
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

 rm(list=ls())

#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# End of Script
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~


